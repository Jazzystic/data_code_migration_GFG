{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfQ3yAdUzZoQ8C4FP+ifUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jazzystic/data_code_migration_GFG/blob/main/migraci%C3%B3n_hist%C3%B3rico_consolidado_GFG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "ZFiSvhCqp1w0",
        "outputId": "9bdb9170-cd73-47b6-faed-ff03b8d6e115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecciona el archivo data_consolidado.csv:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b6f8cbe-7aa1-4f4c-b0f3-bc3931a4638b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b6f8cbe-7aa1-4f4c-b0f3-bc3931a4638b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_consolidado.csv to data_consolidado.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Subir archivo\n",
        "print(\"Selecciona el archivo data_consolidado.csv:\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer el CSV\n",
        "df = pd.read_csv('data_consolidado.csv',\n",
        "                 encoding='utf-8',\n",
        "                 on_bad_lines='warn',\n",
        "                 engine='python')\n",
        "\n",
        "print(f\"Total de filas: {len(df)}\")\n",
        "print(f\"\\nColumnas detectadas ({len(df.columns)}):\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nPrimeras 3 filas:\")\n",
        "print(df.head(3))\n",
        "print(\"\\nValores nulos por columna:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nTipos de datos:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1_BHIN6urjK",
        "outputId": "8c4aaecc-3050-4350-881a-e316126c5980"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de filas: 20638\n",
            "\n",
            "Columnas detectadas (58):\n",
            "['Week', 'WeekDate', 'GFG', 'Audit for:', 'CS', 'Series', 'CS Hotels Names', 'Rank All', 'Total', 'Market', 'TA.Reviews>', 'TA.Excellent', 'TA.Very Good', 'TA.Average', 'TA.Poor', 'TA.Terrible', 'TA.Ratings>', 'TA.Location', 'TA.Cleanliness', 'TA.Service', 'TA.Value', 'GG.Reviews>', 'GG.Excellent', 'GG.Very Good', 'GG.Average', 'GG.Poor', 'GG.Terrible', 'GG.Ratings>', 'GG.Location', 'GG.Cleanliness', 'GG.Service', 'GG.Value', 'EX.Reviews>', 'EX.Excellent', 'EX.Very Good', 'EX.Average', 'EX.Poor', 'EX.Terrible', 'EX.Ratings>', 'EX.Location', 'EX.Cleanliness', 'EX.Service', 'EX.Value', 'BK.Reviews>', 'BK.Excellent', 'BK.Very Good', 'BK.Average', 'BK.Poor', 'BK.Terrible', 'BK.Ratings>', 'BK.Location', 'BK.Cleanliness', 'BK.Service', 'BK.Value', 'Instagram', 'Facebook', 'Twitter', 'Unnamed: 57']\n",
            "\n",
            "Primeras 3 filas:\n",
            "     Week    WeekDate  GFG                           Audit for:      CS  \\\n",
            "0  220801  01/08/2022  Yes  Dreams Bahia Mita Surf & Spa Resort  Direct   \n",
            "1  220801  01/08/2022   No  Dreams Bahia Mita Surf & Spa Resort  Direct   \n",
            "2  220801  01/08/2022   No  Dreams Bahia Mita Surf & Spa Resort  Direct   \n",
            "\n",
            "   Series                             CS Hotels Names  Rank All  Total  \\\n",
            "0     1.0   Dreams Bahia Mita Surf & Spa Resort (363)       5.0   15.0   \n",
            "1     2.0  Secrets Bahia Mita Surf & Spa Resort (278)       6.0   15.0   \n",
            "2     3.0       The St. Regis Punta Mita Resort (120)       7.0   15.0   \n",
            "\n",
            "          Market  ... BK.Terrible BK.Ratings>  BK.Location  BK.Cleanliness  \\\n",
            "0  Punta de Mita  ...         3.0           8          8.2             8.6   \n",
            "1  Punta de Mita  ...         3.0         7.7          7.9             8.9   \n",
            "2  Punta de Mita  ...         NaN         8.5          9.0             9.0   \n",
            "\n",
            "   BK.Service  BK.Value  Instagram  Facebook  Twitter  Unnamed: 57  \n",
            "0         8.6       7.5       5038     10627       38          NaN  \n",
            "1           8       7.4       4933      9766      NaN          NaN  \n",
            "2         9.1         7      44500     48979    15500          NaN  \n",
            "\n",
            "[3 rows x 58 columns]\n",
            "\n",
            "Valores nulos por columna:\n",
            "Week                   0\n",
            "WeekDate               0\n",
            "GFG                    0\n",
            "Audit for:             0\n",
            "CS                    71\n",
            "Series                71\n",
            "CS Hotels Names      181\n",
            "Rank All             181\n",
            "Total                226\n",
            "Market               226\n",
            "TA.Reviews>          190\n",
            "TA.Excellent         190\n",
            "TA.Very Good         207\n",
            "TA.Average           334\n",
            "TA.Poor              450\n",
            "TA.Terrible          532\n",
            "TA.Ratings>          190\n",
            "TA.Location         9150\n",
            "TA.Cleanliness      9151\n",
            "TA.Service          9150\n",
            "TA.Value            9151\n",
            "GG.Reviews>         9124\n",
            "GG.Excellent        9126\n",
            "GG.Very Good        9125\n",
            "GG.Average          9154\n",
            "GG.Poor             9757\n",
            "GG.Terrible         9205\n",
            "GG.Ratings>          194\n",
            "GG.Location         9482\n",
            "GG.Cleanliness      9441\n",
            "GG.Service          9441\n",
            "GG.Value            9128\n",
            "EX.Reviews>         9136\n",
            "EX.Excellent        9144\n",
            "EX.Very Good        9150\n",
            "EX.Average          9136\n",
            "EX.Poor             9203\n",
            "EX.Terrible         9220\n",
            "EX.Ratings>          310\n",
            "EX.Location         9136\n",
            "EX.Cleanliness      9134\n",
            "EX.Service          9134\n",
            "EX.Value            9134\n",
            "BK.Reviews>         9142\n",
            "BK.Excellent        9143\n",
            "BK.Very Good        9150\n",
            "BK.Average          9340\n",
            "BK.Poor             9359\n",
            "BK.Terrible         9744\n",
            "BK.Ratings>          520\n",
            "BK.Location         9192\n",
            "BK.Cleanliness      9192\n",
            "BK.Service          9192\n",
            "BK.Value            9192\n",
            "Instagram           9120\n",
            "Facebook            9120\n",
            "Twitter             9318\n",
            "Unnamed: 57        20638\n",
            "dtype: int64\n",
            "\n",
            "Tipos de datos:\n",
            "Week                 int64\n",
            "WeekDate            object\n",
            "GFG                 object\n",
            "Audit for:          object\n",
            "CS                  object\n",
            "Series             float64\n",
            "CS Hotels Names     object\n",
            "Rank All           float64\n",
            "Total              float64\n",
            "Market              object\n",
            "TA.Reviews>         object\n",
            "TA.Excellent        object\n",
            "TA.Very Good       float64\n",
            "TA.Average         float64\n",
            "TA.Poor            float64\n",
            "TA.Terrible        float64\n",
            "TA.Ratings>        float64\n",
            "TA.Location        float64\n",
            "TA.Cleanliness     float64\n",
            "TA.Service         float64\n",
            "TA.Value           float64\n",
            "GG.Reviews>         object\n",
            "GG.Excellent        object\n",
            "GG.Very Good        object\n",
            "GG.Average         float64\n",
            "GG.Poor            float64\n",
            "GG.Terrible        float64\n",
            "GG.Ratings>        float64\n",
            "GG.Location         object\n",
            "GG.Cleanliness     float64\n",
            "GG.Service         float64\n",
            "GG.Value           float64\n",
            "EX.Reviews>         object\n",
            "EX.Excellent       float64\n",
            "EX.Very Good        object\n",
            "EX.Average          object\n",
            "EX.Poor             object\n",
            "EX.Terrible         object\n",
            "EX.Ratings>        float64\n",
            "EX.Location        float64\n",
            "EX.Cleanliness     float64\n",
            "EX.Service         float64\n",
            "EX.Value           float64\n",
            "BK.Reviews>        float64\n",
            "BK.Excellent       float64\n",
            "BK.Very Good       float64\n",
            "BK.Average         float64\n",
            "BK.Poor            float64\n",
            "BK.Terrible        float64\n",
            "BK.Ratings>         object\n",
            "BK.Location        float64\n",
            "BK.Cleanliness     float64\n",
            "BK.Service          object\n",
            "BK.Value            object\n",
            "Instagram           object\n",
            "Facebook            object\n",
            "Twitter             object\n",
            "Unnamed: 57        float64\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "from psycopg2.extras import execute_batch\n",
        "import pandas as pd\n",
        "\n",
        "# Limpiar datos\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Eliminar columna vacía\n",
        "df_clean = df_clean.drop(columns=['Unnamed: 57'], errors='ignore')\n",
        "\n",
        "# Renombrar columnas para que coincidan con PostgreSQL\n",
        "column_mapping = {\n",
        "    'Week': 'week',\n",
        "    'WeekDate': 'week_date',\n",
        "    'GFG': 'active_selection',\n",
        "    'CS': 'cs',\n",
        "    'Series': 'series',\n",
        "    'CS Hotels Names': 'cs_hotels_names',\n",
        "    'Rank All': 'rank_all',\n",
        "    'Total': 'total',\n",
        "    'Market': 'market',\n",
        "    'TA.Reviews>': 'ta_reviews',\n",
        "    'TA.Excellent': 'ta_excellent',\n",
        "    'TA.Very Good': 'ta_very_good',\n",
        "    'TA.Average': 'ta_average',\n",
        "    'TA.Poor': 'ta_poor',\n",
        "    'TA.Terrible': 'ta_terrible',\n",
        "    'TA.Ratings>': 'ta_ratings',\n",
        "    'TA.Location': 'ta_location',\n",
        "    'TA.Cleanliness': 'ta_cleanliness',\n",
        "    'TA.Service': 'ta_service',\n",
        "    'TA.Value': 'ta_value',\n",
        "    'GG.Reviews>': 'gg_reviews',\n",
        "    'GG.Excellent': 'gg_excellent',\n",
        "    'GG.Very Good': 'gg_very_good',\n",
        "    'GG.Average': 'gg_average',\n",
        "    'GG.Poor': 'gg_poor',\n",
        "    'GG.Terrible': 'gg_terrible',\n",
        "    'GG.Ratings>': 'gg_ratings',\n",
        "    'GG.Location': 'gg_location',\n",
        "    'GG.Cleanliness': 'gg_cleanliness',\n",
        "    'GG.Service': 'gg_service',\n",
        "    'GG.Value': 'gg_value',\n",
        "    'EX.Reviews>': 'ex_reviews',\n",
        "    'EX.Excellent': 'ex_excellent',\n",
        "    'EX.Very Good': 'ex_very_good',\n",
        "    'EX.Average': 'ex_average',\n",
        "    'EX.Poor': 'ex_poor',\n",
        "    'EX.Terrible': 'ex_terrible',\n",
        "    'EX.Ratings>': 'ex_ratings',\n",
        "    'EX.Location': 'ex_location',\n",
        "    'EX.Cleanliness': 'ex_cleanliness',\n",
        "    'EX.Service': 'ex_service',\n",
        "    'EX.Value': 'ex_value',\n",
        "    'BK.Reviews>': 'bk_reviews',\n",
        "    'BK.Excellent': 'bk_excellent',\n",
        "    'BK.Very Good': 'bk_very_good',\n",
        "    'BK.Average': 'bk_average',\n",
        "    'BK.Poor': 'bk_poor',\n",
        "    'BK.Terrible': 'bk_terrible',\n",
        "    'BK.Ratings>': 'bk_ratings',\n",
        "    'BK.Location': 'bk_location',\n",
        "    'BK.Cleanliness': 'bk_cleanliness',\n",
        "    'BK.Service': 'bk_service',\n",
        "    'BK.Value': 'bk_value',\n",
        "    'Instagram': 'instagram',\n",
        "    'Facebook': 'facebook',\n",
        "    'Twitter': 'twitter'\n",
        "}\n",
        "\n",
        "df_clean = df_clean.rename(columns=column_mapping)\n",
        "\n",
        "# Convertir week_date a formato DATE\n",
        "df_clean['week_date'] = pd.to_datetime(df_clean['week_date'], format='%d/%m/%Y', errors='coerce')\n",
        "df_clean['week_date'] = df_clean['week_date'].apply(lambda x: x.date() if pd.notna(x) else None)\n",
        "\n",
        "# Convertir columnas numéricas que DEBEN ser enteros\n",
        "integer_cols = ['week', 'series', 'rank_all', 'total',\n",
        "                'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                'instagram', 'facebook', 'twitter']\n",
        "\n",
        "for col in integer_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "        df_clean[col] = df_clean[col].round(0).astype('Int64')\n",
        "\n",
        "# Convertir columnas DECIMAL\n",
        "decimal_cols = ['ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value']\n",
        "\n",
        "for col in decimal_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "# Reemplazar <NA> con None para PostgreSQL\n",
        "df_clean = df_clean.where(pd.notna(df_clean), None)\n",
        "\n",
        "print(f\"Datos preparados: {len(df_clean)} filas\")\n",
        "print(\"\\nMuestra de datos limpios:\")\n",
        "print(df_clean.head(2))\n",
        "\n",
        "# Conectar a PostgreSQL\n",
        "conn = psycopg2.connect(\n",
        "    host=\"gfg-postgresql-server-2025.postgres.database.azure.com\",\n",
        "    database=\"gfg_asset_management\",\n",
        "    user=\"svc_n8n_admin\",\n",
        "    password=\"PowerBI2025!\",\n",
        "    sslmode=\"require\"\n",
        ")\n",
        "\n",
        "# Preparar datos\n",
        "columns_order = ['week', 'week_date', 'active_selection', 'cs', 'series', 'cs_hotels_names',\n",
        "                 'rank_all', 'total', 'market',\n",
        "                 'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                 'ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                 'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                 'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                 'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                 'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                 'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                 'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value',\n",
        "                 'instagram', 'facebook', 'twitter']\n",
        "\n",
        "data = [tuple(row) for row in df_clean[columns_order].to_numpy()]\n",
        "\n",
        "# SQL de inserción\n",
        "insert_sql = \"\"\"\n",
        "INSERT INTO raw_digital_presence.data_consolidado_historico\n",
        "(week, week_date, active_selection, cs, series, cs_hotels_names,\n",
        " rank_all, total, market,\n",
        " ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible,\n",
        " ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value,\n",
        " gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible,\n",
        " gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value,\n",
        " ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible,\n",
        " ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value,\n",
        " bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible,\n",
        " bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value,\n",
        " instagram, facebook, twitter)\n",
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "# Insertar en batch\n",
        "cursor = conn.cursor()\n",
        "print(\"\\nInsertando datos...\")\n",
        "execute_batch(cursor, insert_sql, data, page_size=1000)\n",
        "conn.commit()\n",
        "\n",
        "print(f\"\\n{len(data)} filas insertadas exitosamente\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\nMigracion completada!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "Wud6U4Gcu_1J",
        "outputId": "af020644-90c2-4a54-a0d8-f5dd69e8519f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot safely cast non-equivalent float64 to int64",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/integer.py\u001b[0m in \u001b[0;36m_safe_cast\u001b[0;34m(cls, values, dtype, copy)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"safe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3730641250.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Convertir columnas DECIMAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# in pandas we don't store numpy str dtypes, so convert to object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# dispatch on extension dtype if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionDtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_array_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36m_from_sequence\u001b[0;34m(cls, scalars, dtype, copy)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_from_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coerce_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/numeric.py\u001b[0m in \u001b[0;36m_coerce_to_array\u001b[0;34m(cls, value, dtype, copy)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mdtype_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_cls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mdefault_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_np_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         values, mask, _, _ = _coerce_to_data_and_mask(\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/numeric.py\u001b[0m in \u001b[0;36m_coerce_to_data_and_mask\u001b[0;34m(values, dtype, copy, dtype_cls, default_dtype)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/integer.py\u001b[0m in \u001b[0;36m_safe_cast\u001b[0;34m(cls, values, dtype, copy)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcasted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;34mf\"cannot safely cast non-equivalent {values.dtype} to {np.dtype(dtype)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             ) from err\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot safely cast non-equivalent float64 to int64"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar duplicados en el CSV\n",
        "duplicados = df_clean.duplicated(subset=['week', 'week_date', 'cs_hotels_names'], keep=False)\n",
        "print(f\"Número de filas duplicadas: {duplicados.sum()}\")\n",
        "\n",
        "if duplicados.sum() > 0:\n",
        "    print(\"\\nPrimeras 10 filas duplicadas:\")\n",
        "    print(df_clean[duplicados][['week', 'week_date', 'cs_hotels_names', 'active_selection']].head(10))\n",
        "\n",
        "    # Eliminar duplicados (mantener la primera ocurrencia)\n",
        "    print(f\"\\nEliminando {duplicados.sum()} duplicados...\")\n",
        "    df_clean = df_clean.drop_duplicates(subset=['week', 'week_date', 'cs_hotels_names'], keep='first')\n",
        "    print(f\"Filas después de eliminar duplicados: {len(df_clean)}\")\n",
        "else:\n",
        "    print(\"No hay duplicados en el CSV\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK6Onad8zZB8",
        "outputId": "26e1ce84-13a1-4604-b5aa-2733afe695eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de filas duplicadas: 8466\n",
            "\n",
            "Primeras 10 filas duplicadas:\n",
            "     week   week_date                             cs_hotels_names  \\\n",
            "0  220801  2022-08-01   Dreams Bahia Mita Surf & Spa Resort (363)   \n",
            "1  220801  2022-08-01  Secrets Bahia Mita Surf & Spa Resort (278)   \n",
            "2  220801  2022-08-01       The St. Regis Punta Mita Resort (120)   \n",
            "3  220801  2022-08-01        Four Seasons Resort Punta Mita (205)   \n",
            "4  220801  2022-08-01                       W Punta de Mita (119)   \n",
            "5  220801  2022-08-01                  Conrad Punta de Mita (324)   \n",
            "6  220801  2022-08-01                                Casa De Mita   \n",
            "7  220801  2022-08-01                     Hotel La Quinta del Sol   \n",
            "8  220801  2022-08-01              Four Seasons Resort Punta Mita   \n",
            "9  220801  2022-08-01       Marival Armony Luxury Resort & Suites   \n",
            "\n",
            "  active_selection  \n",
            "0              Yes  \n",
            "1               No  \n",
            "2               No  \n",
            "3               No  \n",
            "4               No  \n",
            "5               No  \n",
            "6               No  \n",
            "7               No  \n",
            "8               No  \n",
            "9               No  \n",
            "\n",
            "Eliminando 8466 duplicados...\n",
            "Filas después de eliminar duplicados: 15713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recargar el CSV original para ver los duplicados completos\n",
        "df_original = pd.read_csv('data_consolidado.csv', encoding='utf-8', on_bad_lines='warn', engine='python')\n",
        "\n",
        "# Aplicar las mismas transformaciones de nombres\n",
        "df_original = df_original.rename(columns=column_mapping)\n",
        "df_original = df_original.drop(columns=['Unnamed: 57'], errors='ignore')\n",
        "\n",
        "# Encontrar duplicados\n",
        "duplicados = df_original.duplicated(subset=['week', 'week_date', 'cs_hotels_names'], keep=False)\n",
        "\n",
        "print(f\"Total de filas duplicadas: {duplicados.sum()}\")\n",
        "print(\"\\nPrimeras 20 filas duplicadas (TODAS las columnas):\")\n",
        "print(df_original[duplicados].head(20))\n",
        "\n",
        "# Ver si TODAS las columnas son idénticas o solo esas 3\n",
        "print(\"\\n¿Son filas COMPLETAMENTE idénticas?\")\n",
        "duplicados_completos = df_original.duplicated(keep=False)\n",
        "print(f\"Filas completamente idénticas: {duplicados_completos.sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qllq1Xb0Wpm",
        "outputId": "59ae48d1-d820-4fab-c0ce-0e90f228b5a8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de filas duplicadas: 8466\n",
            "\n",
            "Primeras 20 filas duplicadas (TODAS las columnas):\n",
            "      week   week_date active_selection                           Audit for:  \\\n",
            "0   220801  01/08/2022              Yes  Dreams Bahia Mita Surf & Spa Resort   \n",
            "1   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "2   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "3   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "4   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "5   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "6   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "7   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "8   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "9   220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "10  220801  01/08/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "11  220725  25/07/2022              Yes  Dreams Bahia Mita Surf & Spa Resort   \n",
            "12  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "13  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "14  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "15  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "16  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "17  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "18  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "19  220725  25/07/2022               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "\n",
            "        cs  series                               cs_hotels_names  rank_all  \\\n",
            "0   Direct     1.0     Dreams Bahia Mita Surf & Spa Resort (363)       5.0   \n",
            "1   Direct     2.0    Secrets Bahia Mita Surf & Spa Resort (278)       6.0   \n",
            "2   Direct     3.0         The St. Regis Punta Mita Resort (120)       7.0   \n",
            "3   Direct     4.0          Four Seasons Resort Punta Mita (205)       3.0   \n",
            "4   Direct     5.0                         W Punta de Mita (119)      10.0   \n",
            "5   Direct     6.0                    Conrad Punta de Mita (324)       8.0   \n",
            "6    Top 5     7.0                                  Casa De Mita       1.0   \n",
            "7    Top 5     8.0                       Hotel La Quinta del Sol       2.0   \n",
            "8    Top 5     9.0                Four Seasons Resort Punta Mita       3.0   \n",
            "9    Top 5    10.0         Marival Armony Luxury Resort & Suites       4.0   \n",
            "10   Top 5    11.0  Dreams Bahia Mita Surf & Spa Resort ( 363 )        5.0   \n",
            "11  Direct     1.0     Dreams Bahia Mita Surf & Spa Resort (363)       5.0   \n",
            "12  Direct     2.0    Secrets Bahia Mita Surf & Spa Resort (278)       6.0   \n",
            "13  Direct     3.0         The St. Regis Punta Mita Resort (120)       7.0   \n",
            "14  Direct     4.0          Four Seasons Resort Punta Mita (205)       3.0   \n",
            "15  Direct     5.0                         W Punta de Mita (119)      10.0   \n",
            "16  Direct     6.0                    Conrad Punta de Mita (324)       8.0   \n",
            "17   Top 5     7.0                                  Casa De Mita       1.0   \n",
            "18   Top 5     8.0                       Hotel La Quinta del Sol       2.0   \n",
            "19   Top 5     9.0                Four Seasons Resort Punta Mita       3.0   \n",
            "\n",
            "    total         market  ... bk_poor bk_terrible  bk_ratings  bk_location  \\\n",
            "0    15.0  Punta de Mita  ...     5.0         3.0           8          8.2   \n",
            "1    15.0  Punta de Mita  ...     2.0         3.0         7.7          7.9   \n",
            "2    15.0  Punta de Mita  ...     2.0         NaN         8.5          9.0   \n",
            "3    15.0  Punta de Mita  ...     NaN         NaN         9.5          9.6   \n",
            "4    15.0  Punta de Mita  ...     2.0         4.0         8.5          9.1   \n",
            "5    15.0  Punta de Mita  ...     1.0         NaN         9.1          9.1   \n",
            "6    15.0  Punta de Mita  ...     NaN         NaN         NaN          NaN   \n",
            "7    15.0  Punta de Mita  ...     NaN         NaN           9          NaN   \n",
            "8    15.0  Punta de Mita  ...     NaN         NaN         9.5          NaN   \n",
            "9    15.0  Punta de Mita  ...     NaN         NaN         7.8          NaN   \n",
            "10   15.0  Punta de Mita  ...     NaN         NaN         7.7          NaN   \n",
            "11   15.0  Punta de Mita  ...     5.0         3.0           8          8.2   \n",
            "12   15.0  Punta de Mita  ...     2.0         3.0         7.6          8.0   \n",
            "13   15.0  Punta de Mita  ...     2.0         NaN         8.5          9.0   \n",
            "14   15.0  Punta de Mita  ...     NaN         NaN         9.5          9.6   \n",
            "15   15.0  Punta de Mita  ...     2.0         4.0         8.5          9.1   \n",
            "16   15.0  Punta de Mita  ...     1.0         NaN         9.1          9.1   \n",
            "17   15.0  Punta de Mita  ...     NaN         NaN         NaN          NaN   \n",
            "18   15.0  Punta de Mita  ...     NaN         NaN           9          NaN   \n",
            "19   15.0  Punta de Mita  ...     NaN         NaN         9.5          NaN   \n",
            "\n",
            "    bk_cleanliness  bk_service  bk_value  instagram  facebook  twitter  \n",
            "0              8.6         8.6       7.5       5038     10627       38  \n",
            "1              8.9           8       7.4       4933      9766      NaN  \n",
            "2              9.0         9.1         7      44500     48979    15500  \n",
            "3              9.6         9.6       8.1      95700    143825    17900  \n",
            "4              9.2         9.2       7.4      74700     48582     1392  \n",
            "5              9.5         9.2         8      24800     14382        0  \n",
            "6              NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "7              NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "8              NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "9              NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "10             NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "11             8.6         8.6       7.5       4916     10627       38  \n",
            "12             8.9           8       7.3       4709      9766      NaN  \n",
            "13             9.0         9.1         7      44500     48979    15500  \n",
            "14             9.6         9.6       8.2      95600    143825    17900  \n",
            "15             9.2         9.2       7.3      74700     48582     1391  \n",
            "16             9.5         9.2         8      24700     14382        0  \n",
            "17             NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "18             NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "19             NaN         NaN       NaN        NaN       NaN      NaN  \n",
            "\n",
            "[20 rows x 57 columns]\n",
            "\n",
            "¿Son filas COMPLETAMENTE idénticas?\n",
            "Filas completamente idénticas: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar datos\n",
        "columns_order = ['week', 'week_date', 'active_selection', 'cs', 'series', 'cs_hotels_names',\n",
        "                 'rank_all', 'total', 'market',\n",
        "                 'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                 'ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                 'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                 'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                 'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                 'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                 'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                 'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value',\n",
        "                 'instagram', 'facebook', 'twitter']\n",
        "\n",
        "# Convertir DataFrame a lista de tuplas\n",
        "data = []\n",
        "for _, row in df_clean[columns_order].iterrows():\n",
        "    tuple_data = []\n",
        "    for val in row:\n",
        "        if pd.isna(val) or val is pd.NA:\n",
        "            tuple_data.append(None)\n",
        "        else:\n",
        "            tuple_data.append(val)\n",
        "    data.append(tuple(tuple_data))\n",
        "\n",
        "# Conectar a PostgreSQL\n",
        "conn = psycopg2.connect(\n",
        "    host=\"gfg-postgresql-server-2025.postgres.database.azure.com\",\n",
        "    database=\"gfg_asset_management\",\n",
        "    user=\"svc_n8n_admin\",\n",
        "    password=\"PowerBI2025!\",\n",
        "    sslmode=\"require\"\n",
        ")\n",
        "\n",
        "# SQL de inserción\n",
        "insert_sql = \"\"\"\n",
        "INSERT INTO raw_digital_presence.data_consolidado_historico\n",
        "(week, week_date, active_selection, cs, series, cs_hotels_names,\n",
        " rank_all, total, market,\n",
        " ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible,\n",
        " ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value,\n",
        " gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible,\n",
        " gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value,\n",
        " ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible,\n",
        " ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value,\n",
        " bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible,\n",
        " bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value,\n",
        " instagram, facebook, twitter)\n",
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "# Insertar en batch\n",
        "cursor = conn.cursor()\n",
        "print(\"Insertando datos...\")\n",
        "execute_batch(cursor, insert_sql, data, page_size=1000)\n",
        "conn.commit()\n",
        "\n",
        "print(f\"{len(data)} filas insertadas exitosamente\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"Migracion completada!\")"
      ],
      "metadata": {
        "id": "9zxGp7D6zrlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "from psycopg2.extras import execute_batch\n",
        "import pandas as pd\n",
        "\n",
        "# Limpiar datos\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Eliminar columna vacía\n",
        "df_clean = df_clean.drop(columns=['Unnamed: 57'], errors='ignore')\n",
        "\n",
        "# Renombrar columnas para que coincidan con PostgreSQL\n",
        "column_mapping = {\n",
        "    'Week': 'week',\n",
        "    'WeekDate': 'week_date',\n",
        "    'GFG': 'active_selection',\n",
        "    'CS': 'cs',\n",
        "    'Series': 'series',\n",
        "    'CS Hotels Names': 'cs_hotels_names',\n",
        "    'Rank All': 'rank_all',\n",
        "    'Total': 'total',\n",
        "    'Market': 'market',\n",
        "    'TA.Reviews>': 'ta_reviews',\n",
        "    'TA.Excellent': 'ta_excellent',\n",
        "    'TA.Very Good': 'ta_very_good',\n",
        "    'TA.Average': 'ta_average',\n",
        "    'TA.Poor': 'ta_poor',\n",
        "    'TA.Terrible': 'ta_terrible',\n",
        "    'TA.Ratings>': 'ta_ratings',\n",
        "    'TA.Location': 'ta_location',\n",
        "    'TA.Cleanliness': 'ta_cleanliness',\n",
        "    'TA.Service': 'ta_service',\n",
        "    'TA.Value': 'ta_value',\n",
        "    'GG.Reviews>': 'gg_reviews',\n",
        "    'GG.Excellent': 'gg_excellent',\n",
        "    'GG.Very Good': 'gg_very_good',\n",
        "    'GG.Average': 'gg_average',\n",
        "    'GG.Poor': 'gg_poor',\n",
        "    'GG.Terrible': 'gg_terrible',\n",
        "    'GG.Ratings>': 'gg_ratings',\n",
        "    'GG.Location': 'gg_location',\n",
        "    'GG.Cleanliness': 'gg_cleanliness',\n",
        "    'GG.Service': 'gg_service',\n",
        "    'GG.Value': 'gg_value',\n",
        "    'EX.Reviews>': 'ex_reviews',\n",
        "    'EX.Excellent': 'ex_excellent',\n",
        "    'EX.Very Good': 'ex_very_good',\n",
        "    'EX.Average': 'ex_average',\n",
        "    'EX.Poor': 'ex_poor',\n",
        "    'EX.Terrible': 'ex_terrible',\n",
        "    'EX.Ratings>': 'ex_ratings',\n",
        "    'EX.Location': 'ex_location',\n",
        "    'EX.Cleanliness': 'ex_cleanliness',\n",
        "    'EX.Service': 'ex_service',\n",
        "    'EX.Value': 'ex_value',\n",
        "    'BK.Reviews>': 'bk_reviews',\n",
        "    'BK.Excellent': 'bk_excellent',\n",
        "    'BK.Very Good': 'bk_very_good',\n",
        "    'BK.Average': 'bk_average',\n",
        "    'BK.Poor': 'bk_poor',\n",
        "    'BK.Terrible': 'bk_terrible',\n",
        "    'BK.Ratings>': 'bk_ratings',\n",
        "    'BK.Location': 'bk_location',\n",
        "    'BK.Cleanliness': 'bk_cleanliness',\n",
        "    'BK.Service': 'bk_service',\n",
        "    'BK.Value': 'bk_value',\n",
        "    'Instagram': 'instagram',\n",
        "    'Facebook': 'facebook',\n",
        "    'Twitter': 'twitter'\n",
        "}\n",
        "\n",
        "df_clean = df_clean.rename(columns=column_mapping)\n",
        "\n",
        "# Convertir week_date a formato DATE\n",
        "df_clean['week_date'] = pd.to_datetime(df_clean['week_date'], format='%d/%m/%Y', errors='coerce')\n",
        "df_clean['week_date'] = df_clean['week_date'].apply(lambda x: x.date() if pd.notna(x) else None)\n",
        "\n",
        "# Convertir columnas numéricas que DEBEN ser enteros\n",
        "integer_cols = ['week', 'series', 'rank_all', 'total',\n",
        "                'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                'instagram', 'facebook', 'twitter']\n",
        "\n",
        "for col in integer_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "        df_clean[col] = df_clean[col].round(0).astype('Int64')\n",
        "\n",
        "# Convertir columnas DECIMAL\n",
        "decimal_cols = ['ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value']\n",
        "\n",
        "for col in decimal_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "print(f\"Datos preparados: {len(df_clean)} filas\")\n",
        "print(\"\\nMuestra de datos limpios:\")\n",
        "print(df_clean.head(2))\n",
        "\n",
        "# Conectar a PostgreSQL\n",
        "conn = psycopg2.connect(\n",
        "    host=\"gfg-postgresql-server-2025.postgres.database.azure.com\",\n",
        "    database=\"gfg_asset_management\",\n",
        "    user=\"svc_n8n_admin\",\n",
        "    password=\"PowerBI2025!\",\n",
        "    sslmode=\"require\"\n",
        ")\n",
        "\n",
        "# Preparar datos\n",
        "columns_order = ['week', 'week_date', 'active_selection', 'cs', 'series', 'cs_hotels_names',\n",
        "                 'rank_all', 'total', 'market',\n",
        "                 'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                 'ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                 'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                 'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                 'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                 'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                 'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                 'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value',\n",
        "                 'instagram', 'facebook', 'twitter']\n",
        "\n",
        "# Convertir DataFrame a lista de tuplas manejando pd.NA correctamente\n",
        "data = []\n",
        "for _, row in df_clean[columns_order].iterrows():\n",
        "    tuple_data = []\n",
        "    for val in row:\n",
        "        if pd.isna(val) or val is pd.NA:\n",
        "            tuple_data.append(None)\n",
        "        else:\n",
        "            tuple_data.append(val)\n",
        "    data.append(tuple(tuple_data))\n",
        "\n",
        "# SQL de inserción\n",
        "insert_sql = \"\"\"\n",
        "INSERT INTO raw_digital_presence.data_consolidado_historico\n",
        "(week, week_date, active_selection, cs, series, cs_hotels_names,\n",
        " rank_all, total, market,\n",
        " ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible,\n",
        " ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value,\n",
        " gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible,\n",
        " gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value,\n",
        " ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible,\n",
        " ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value,\n",
        " bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible,\n",
        " bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value,\n",
        " instagram, facebook, twitter)\n",
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "# Insertar en batch\n",
        "cursor = conn.cursor()\n",
        "print(\"\\nInsertando datos...\")\n",
        "execute_batch(cursor, insert_sql, data, page_size=1000)\n",
        "conn.commit()\n",
        "\n",
        "print(f\"\\n{len(data)} filas insertadas exitosamente\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\nMigracion completada!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "SokujsA7wJjP",
        "outputId": "d8fac88b-72c1-4ac3-f639-b72773aa804f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos preparados: 20638 filas\n",
            "\n",
            "Muestra de datos limpios:\n",
            "     week   week_date active_selection                           Audit for:  \\\n",
            "0  220801  2022-08-01              Yes  Dreams Bahia Mita Surf & Spa Resort   \n",
            "1  220801  2022-08-01               No  Dreams Bahia Mita Surf & Spa Resort   \n",
            "\n",
            "       cs  series                             cs_hotels_names  rank_all  \\\n",
            "0  Direct       1   Dreams Bahia Mita Surf & Spa Resort (363)         5   \n",
            "1  Direct       2  Secrets Bahia Mita Surf & Spa Resort (278)         6   \n",
            "\n",
            "   total         market  ...  bk_poor  bk_terrible  bk_ratings  bk_location  \\\n",
            "0     15  Punta de Mita  ...        5            3         8.0          8.2   \n",
            "1     15  Punta de Mita  ...        2            3         7.7          7.9   \n",
            "\n",
            "   bk_cleanliness  bk_service  bk_value  instagram  facebook  twitter  \n",
            "0             8.6         8.6       7.5       5038     10627       38  \n",
            "1             8.9         8.0       7.4       4933      9766     <NA>  \n",
            "\n",
            "[2 rows x 57 columns]\n",
            "\n",
            "Insertando datos...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UniqueViolation",
          "evalue": "duplicate key value violates unique constraint \"data_consolidado_historico_week_week_date_cs_hotels_names_key\"\nDETAIL:  Key (week, week_date, cs_hotels_names)=(211018, 2021-10-18, Dreams Bahia Mita Surf & Spa Resort (363)) already exists.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1383117007.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInsertando datos...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mexecute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/psycopg2/extras.py\u001b[0m in \u001b[0;36mexecute_batch\u001b[0;34m(cur, sql, argslist, page_size)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_paginate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margslist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0msqls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m         \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"data_consolidado_historico_week_week_date_cs_hotels_names_key\"\nDETAIL:  Key (week, week_date, cs_hotels_names)=(211018, 2021-10-18, Dreams Bahia Mita Surf & Spa Resort (363)) already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recargar CSV desde cero\n",
        "df = pd.read_csv('data_consolidado.csv', encoding='utf-8', on_bad_lines='warn', engine='python')\n",
        "\n",
        "# Aplicar todas las transformaciones SIN eliminar \"duplicados\"\n",
        "# (todo el código de limpieza anterior EXCEPTO la parte de drop_duplicates)"
      ],
      "metadata": {
        "id": "eywZ1rDfzXOn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "from psycopg2.extras import execute_batch\n",
        "import pandas as pd\n",
        "\n",
        "# Recargar CSV completo desde cero\n",
        "df = pd.read_csv('data_consolidado.csv',\n",
        "                 encoding='utf-8',\n",
        "                 on_bad_lines='warn',\n",
        "                 engine='python')\n",
        "\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Eliminar columna vacía\n",
        "df_clean = df_clean.drop(columns=['Unnamed: 57'], errors='ignore')\n",
        "\n",
        "# Renombrar columnas\n",
        "column_mapping = {\n",
        "    'Week': 'week', 'WeekDate': 'week_date', 'GFG': 'active_selection',\n",
        "    'CS': 'cs', 'Series': 'series', 'CS Hotels Names': 'cs_hotels_names',\n",
        "    'Rank All': 'rank_all', 'Total': 'total', 'Market': 'market',\n",
        "    'TA.Reviews>': 'ta_reviews', 'TA.Excellent': 'ta_excellent', 'TA.Very Good': 'ta_very_good',\n",
        "    'TA.Average': 'ta_average', 'TA.Poor': 'ta_poor', 'TA.Terrible': 'ta_terrible',\n",
        "    'TA.Ratings>': 'ta_ratings', 'TA.Location': 'ta_location', 'TA.Cleanliness': 'ta_cleanliness',\n",
        "    'TA.Service': 'ta_service', 'TA.Value': 'ta_value',\n",
        "    'GG.Reviews>': 'gg_reviews', 'GG.Excellent': 'gg_excellent', 'GG.Very Good': 'gg_very_good',\n",
        "    'GG.Average': 'gg_average', 'GG.Poor': 'gg_poor', 'GG.Terrible': 'gg_terrible',\n",
        "    'GG.Ratings>': 'gg_ratings', 'GG.Location': 'gg_location', 'GG.Cleanliness': 'gg_cleanliness',\n",
        "    'GG.Service': 'gg_service', 'GG.Value': 'gg_value',\n",
        "    'EX.Reviews>': 'ex_reviews', 'EX.Excellent': 'ex_excellent', 'EX.Very Good': 'ex_very_good',\n",
        "    'EX.Average': 'ex_average', 'EX.Poor': 'ex_poor', 'EX.Terrible': 'ex_terrible',\n",
        "    'EX.Ratings>': 'ex_ratings', 'EX.Location': 'ex_location', 'EX.Cleanliness': 'ex_cleanliness',\n",
        "    'EX.Service': 'ex_service', 'EX.Value': 'ex_value',\n",
        "    'BK.Reviews>': 'bk_reviews', 'BK.Excellent': 'bk_excellent', 'BK.Very Good': 'bk_very_good',\n",
        "    'BK.Average': 'bk_average', 'BK.Poor': 'bk_poor', 'BK.Terrible': 'bk_terrible',\n",
        "    'BK.Ratings>': 'bk_ratings', 'BK.Location': 'bk_location', 'BK.Cleanliness': 'bk_cleanliness',\n",
        "    'BK.Service': 'bk_service', 'BK.Value': 'bk_value',\n",
        "    'Instagram': 'instagram', 'Facebook': 'facebook', 'Twitter': 'twitter'\n",
        "}\n",
        "\n",
        "df_clean = df_clean.rename(columns=column_mapping)\n",
        "\n",
        "# Convertir fechas\n",
        "df_clean['week_date'] = pd.to_datetime(df_clean['week_date'], format='%d/%m/%Y', errors='coerce')\n",
        "df_clean['week_date'] = df_clean['week_date'].apply(lambda x: x.date() if pd.notna(x) else None)\n",
        "\n",
        "# Convertir enteros\n",
        "integer_cols = ['week', 'series', 'rank_all', 'total',\n",
        "                'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                'instagram', 'facebook', 'twitter']\n",
        "\n",
        "for col in integer_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce').round(0).astype('Int64')\n",
        "\n",
        "# Convertir decimales\n",
        "decimal_cols = ['ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value']\n",
        "\n",
        "for col in decimal_cols:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "print(f\"Datos preparados: {len(df_clean)} filas\")\n",
        "\n",
        "# Conectar\n",
        "conn = psycopg2.connect(\n",
        "    host=\"gfg-postgresql-server-2025.postgres.database.azure.com\",\n",
        "    database=\"gfg_asset_management\",\n",
        "    user=\"svc_n8n_admin\",\n",
        "    password=\"PowerBI2025!\",\n",
        "    sslmode=\"require\"\n",
        ")\n",
        "\n",
        "columns_order = ['week', 'week_date', 'active_selection', 'cs', 'series', 'cs_hotels_names',\n",
        "                 'rank_all', 'total', 'market',\n",
        "                 'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                 'ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                 'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                 'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                 'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                 'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                 'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                 'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value',\n",
        "                 'instagram', 'facebook', 'twitter']\n",
        "\n",
        "data = []\n",
        "for _, row in df_clean[columns_order].iterrows():\n",
        "    tuple_data = []\n",
        "    for val in row:\n",
        "        if pd.isna(val) or val is pd.NA:\n",
        "            tuple_data.append(None)\n",
        "        else:\n",
        "            tuple_data.append(val)\n",
        "    data.append(tuple(tuple_data))\n",
        "\n",
        "insert_sql = \"\"\"\n",
        "INSERT INTO raw_digital_presence.data_consolidado_historico\n",
        "(week, week_date, active_selection, cs, series, cs_hotels_names,\n",
        " rank_all, total, market,\n",
        " ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible,\n",
        " ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value,\n",
        " gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible,\n",
        " gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value,\n",
        " ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible,\n",
        " ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value,\n",
        " bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible,\n",
        " bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value,\n",
        " instagram, facebook, twitter)\n",
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "cursor = conn.cursor()\n",
        "print(\"Insertando datos...\")\n",
        "execute_batch(cursor, insert_sql, data, page_size=1000)\n",
        "conn.commit()\n",
        "\n",
        "print(f\"{len(data)} filas insertadas exitosamente\")\n",
        "\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"Migracion completada!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "LO2WVO991ppE",
        "outputId": "952288a5-7c29-4352-bc01-f8a3e6e097cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos preparados: 20638 filas\n",
            "Insertando datos...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UniqueViolation",
          "evalue": "duplicate key value violates unique constraint \"data_consolidado_historico_unique_key\"\nDETAIL:  Key (week, week_date, cs_hotels_names, series)=(211018, 2021-10-18, The St. Regis Punta Mita Resort (120), 3) already exists.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-481092318.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Insertando datos...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mexecute_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/psycopg2/extras.py\u001b[0m in \u001b[0;36mexecute_batch\u001b[0;34m(cur, sql, argslist, page_size)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_paginate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margslist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0msqls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m         \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"data_consolidado_historico_unique_key\"\nDETAIL:  Key (week, week_date, cs_hotels_names, series)=(211018, 2021-10-18, The St. Regis Punta Mita Resort (120), 3) already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar duplicados con las 4 columnas del constraint\n",
        "duplicados_reales = df_clean.duplicated(subset=['week', 'week_date', 'cs_hotels_names', 'series'], keep=False)\n",
        "\n",
        "print(f\"Filas duplicadas (considerando series): {duplicados_reales.sum()}\")\n",
        "\n",
        "if duplicados_reales.sum() > 0:\n",
        "    print(\"\\nPrimeras 20 filas duplicadas:\")\n",
        "    print(df_clean[duplicados_reales][['week', 'week_date', 'cs_hotels_names', 'series', 'rank_all']].head(20))\n",
        "\n",
        "    # Ver si son COMPLETAMENTE idénticas\n",
        "    duplicados_completos = df_clean.duplicated(keep=False)\n",
        "    print(f\"\\nFilas COMPLETAMENTE idénticas: {duplicados_completos.sum()}\")\n",
        "\n",
        "    # Eliminar duplicados manteniendo la primera ocurrencia\n",
        "    print(\"\\nEliminando duplicados...\")\n",
        "    df_clean = df_clean.drop_duplicates(subset=['week', 'week_date', 'cs_hotels_names', 'series'], keep='first')\n",
        "    print(f\"Filas después de limpiar: {len(df_clean)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsOE-FcL2FMS",
        "outputId": "141b2363-71b1-47d4-95fe-77edfc9f97b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filas duplicadas (considerando series): 6969\n",
            "\n",
            "Primeras 20 filas duplicadas:\n",
            "      week   week_date                               cs_hotels_names  series  \\\n",
            "1   220801  2022-08-01    Secrets Bahia Mita Surf & Spa Resort (278)       2   \n",
            "2   220801  2022-08-01         The St. Regis Punta Mita Resort (120)       3   \n",
            "3   220801  2022-08-01          Four Seasons Resort Punta Mita (205)       4   \n",
            "4   220801  2022-08-01                         W Punta de Mita (119)       5   \n",
            "5   220801  2022-08-01                    Conrad Punta de Mita (324)       6   \n",
            "6   220801  2022-08-01                                  Casa De Mita       7   \n",
            "7   220801  2022-08-01                       Hotel La Quinta del Sol       8   \n",
            "8   220801  2022-08-01                Four Seasons Resort Punta Mita       9   \n",
            "9   220801  2022-08-01         Marival Armony Luxury Resort & Suites      10   \n",
            "10  220801  2022-08-01  Dreams Bahia Mita Surf & Spa Resort ( 363 )       11   \n",
            "12  220725  2022-07-25    Secrets Bahia Mita Surf & Spa Resort (278)       2   \n",
            "13  220725  2022-07-25         The St. Regis Punta Mita Resort (120)       3   \n",
            "14  220725  2022-07-25          Four Seasons Resort Punta Mita (205)       4   \n",
            "15  220725  2022-07-25                         W Punta de Mita (119)       5   \n",
            "16  220725  2022-07-25                    Conrad Punta de Mita (324)       6   \n",
            "17  220725  2022-07-25                                  Casa De Mita       7   \n",
            "18  220725  2022-07-25                       Hotel La Quinta del Sol       8   \n",
            "19  220725  2022-07-25                Four Seasons Resort Punta Mita       9   \n",
            "20  220725  2022-07-25         Marival Armony Luxury Resort & Suites      10   \n",
            "21  220725  2022-07-25  Dreams Bahia Mita Surf & Spa Resort ( 363 )       11   \n",
            "\n",
            "    rank_all  \n",
            "1          6  \n",
            "2          7  \n",
            "3          3  \n",
            "4         10  \n",
            "5          8  \n",
            "6          1  \n",
            "7          2  \n",
            "8          3  \n",
            "9          4  \n",
            "10         5  \n",
            "12         6  \n",
            "13         7  \n",
            "14         3  \n",
            "15        10  \n",
            "16         8  \n",
            "17         1  \n",
            "18         2  \n",
            "19         3  \n",
            "20         4  \n",
            "21         5  \n",
            "\n",
            "Filas COMPLETAMENTE idénticas: 0\n",
            "\n",
            "Eliminando duplicados...\n",
            "Filas después de limpiar: 16708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificación exhaustiva de duplicados\n",
        "print(\"=== ANÁLISIS DE DUPLICADOS ===\\n\")\n",
        "\n",
        "# 1. Ver filas duplicadas completas\n",
        "duplicados_4col = df_clean.duplicated(subset=['week', 'week_date', 'cs_hotels_names', 'series'], keep=False)\n",
        "df_duplicados = df_clean[duplicados_4col].sort_values(['week', 'week_date', 'cs_hotels_names', 'series'])\n",
        "\n",
        "print(f\"Total filas marcadas como duplicadas: {duplicados_4col.sum()}\")\n",
        "\n",
        "# 2. Tomar un caso específico para analizar\n",
        "caso_ejemplo = df_duplicados.head(2)\n",
        "print(\"\\n=== EJEMPLO: Primeras 2 filas duplicadas (TODAS las columnas) ===\")\n",
        "print(caso_ejemplo.to_string())\n",
        "\n",
        "# 3. Ver si son IDÉNTICAS en todas las columnas\n",
        "print(\"\\n=== ¿Son COMPLETAMENTE idénticas? ===\")\n",
        "duplicados_completos = df_clean.duplicated(keep=False)\n",
        "print(f\"Filas 100% idénticas en TODAS las columnas: {duplicados_completos.sum()}\")\n",
        "\n",
        "# 4. Si NO son idénticas, ver qué columnas difieren\n",
        "if duplicados_completos.sum() == 0 and duplicados_4col.sum() > 0:\n",
        "    print(\"\\n⚠️ ALERTA: Las filas NO son idénticas - difieren en otras columnas\")\n",
        "    print(\"\\nMostrando diferencias en las primeras 10 filas duplicadas:\")\n",
        "\n",
        "    # Agrupar por las 4 columnas clave y contar cuántas filas hay por grupo\n",
        "    grupos = df_clean[duplicados_4col].groupby(['week', 'week_date', 'cs_hotels_names', 'series']).size()\n",
        "    print(f\"\\nGrupos de duplicados: {len(grupos)}\")\n",
        "    print(f\"Filas por grupo (primeros 5):\")\n",
        "    print(grupos.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm3vVPNm2cds",
        "outputId": "cf5326d4-feb7-4d1b-f458-d90c51897de9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ANÁLISIS DE DUPLICADOS ===\n",
            "\n",
            "Total filas marcadas como duplicadas: 0\n",
            "\n",
            "=== EJEMPLO: Primeras 2 filas duplicadas (TODAS las columnas) ===\n",
            "Empty DataFrame\n",
            "Columns: [week, week_date, active_selection, Audit for:, cs, series, cs_hotels_names, rank_all, total, market, ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible, ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value, gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible, gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value, ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible, ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value, bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible, bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value, instagram, facebook, twitter]\n",
            "Index: []\n",
            "\n",
            "=== ¿Son COMPLETAMENTE idénticas? ===\n",
            "Filas 100% idénticas en TODAS las columnas: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recargar CSV ORIGINAL sin tocar\n",
        "df_original = pd.read_csv('data_consolidado.csv',\n",
        "                          encoding='utf-8',\n",
        "                          on_bad_lines='warn',\n",
        "                          engine='python')\n",
        "\n",
        "# Aplicar solo renombrado de columnas (sin eliminar nada)\n",
        "column_mapping = {\n",
        "    'Week': 'week', 'WeekDate': 'week_date', 'GFG': 'active_selection',\n",
        "    'CS': 'cs', 'Series': 'series', 'CS Hotels Names': 'cs_hotels_names',\n",
        "    'Rank All': 'rank_all', 'Total': 'total', 'Market': 'market'\n",
        "}\n",
        "\n",
        "df_check = df_original.rename(columns=column_mapping)\n",
        "\n",
        "# Convertir fechas para comparación correcta\n",
        "df_check['week_date'] = pd.to_datetime(df_check['week_date'], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "# Buscar duplicados basados en las 4 columnas clave\n",
        "duplicados = df_check.duplicated(subset=['week', 'week_date', 'cs_hotels_names', 'series'], keep=False)\n",
        "\n",
        "print(f\"Total filas duplicadas: {duplicados.sum()}\")\n",
        "\n",
        "if duplicados.sum() > 0:\n",
        "    # Ver las primeras 10 filas duplicadas CON TODAS LAS COLUMNAS\n",
        "    df_dup = df_check[duplicados].sort_values(['week', 'week_date', 'cs_hotels_names', 'series'])\n",
        "\n",
        "    print(\"\\n=== Primeras 10 filas duplicadas (TODAS las columnas) ===\")\n",
        "    print(df_dup[['week', 'week_date', 'cs_hotels_names', 'series', 'rank_all',\n",
        "                  'TA.Reviews>', 'TA.Ratings>', 'Instagram', 'Facebook']].head(10))\n",
        "\n",
        "    # Verificar si son COMPLETAMENTE idénticas\n",
        "    duplicados_completos = df_check.duplicated(keep=False)\n",
        "    print(f\"\\n¿Son 100% idénticas en TODAS las columnas?: {duplicados_completos.sum()} filas\")\n",
        "\n",
        "    if duplicados_completos.sum() == duplicados.sum():\n",
        "        print(\"✅ SÍ - Son duplicados reales (todas las columnas idénticas)\")\n",
        "    else:\n",
        "        print(\"⚠️ NO - Solo comparten las 4 columnas clave, pero difieren en otras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGoGxBWd2xR6",
        "outputId": "3a4e0ca6-db22-44b8-ff8d-9d5448882ed1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total filas duplicadas: 6969\n",
            "\n",
            "=== Primeras 10 filas duplicadas (TODAS las columnas) ===\n",
            "        week  week_date                 cs_hotels_names  series  rank_all  \\\n",
            "435   211018 2021-10-18                    Casa De Mita     7.0       1.0   \n",
            "446   211018 2021-10-18                    Casa De Mita     7.0       1.0   \n",
            "6925  211018 2021-10-18                    Casa De Mita     7.0       1.0   \n",
            "438   211018 2021-10-18            Conrad Punta de Mita    10.0       4.0   \n",
            "449   211018 2021-10-18            Conrad Punta de Mita    10.0       4.0   \n",
            "6928  211018 2021-10-18            Conrad Punta de Mita    10.0       4.0   \n",
            "434   211018 2021-10-18      Conrad Punta de Mita (324)     6.0       4.0   \n",
            "445   211018 2021-10-18      Conrad Punta de Mita (324)     6.0       4.0   \n",
            "6924  211018 2021-10-18      Conrad Punta de Mita (324)     6.0       4.0   \n",
            "439   211018 2021-10-18  Four Seasons Resort Punta Mita    11.0       5.0   \n",
            "\n",
            "     TA.Reviews>  TA.Ratings> Instagram Facebook  \n",
            "435          214          5.0       NaN      NaN  \n",
            "446          214          5.0       NaN      NaN  \n",
            "6925         214          5.0       NaN      NaN  \n",
            "438          179          4.5       NaN      NaN  \n",
            "449          179          4.5       NaN      NaN  \n",
            "6928         179          4.5       NaN      NaN  \n",
            "434          179          4.5     14500    12981  \n",
            "445          179          4.5     14500    12981  \n",
            "6924         179          4.5     14500    12981  \n",
            "439         2665          4.5       NaN      NaN  \n",
            "\n",
            "¿Son 100% idénticas en TODAS las columnas?: 0 filas\n",
            "⚠️ NO - Solo comparten las 4 columnas clave, pero difieren en otras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2\n",
        "from psycopg2.extras import execute_batch\n",
        "\n",
        "# df_clean ya tiene las 16,708 filas sin duplicados\n",
        "\n",
        "# Preparar datos\n",
        "columns_order = ['week', 'week_date', 'active_selection', 'cs', 'series', 'cs_hotels_names',\n",
        "                 'rank_all', 'total', 'market',\n",
        "                 'ta_reviews', 'ta_excellent', 'ta_very_good', 'ta_average', 'ta_poor', 'ta_terrible',\n",
        "                 'ta_ratings', 'ta_location', 'ta_cleanliness', 'ta_service', 'ta_value',\n",
        "                 'gg_reviews', 'gg_excellent', 'gg_very_good', 'gg_average', 'gg_poor', 'gg_terrible',\n",
        "                 'gg_ratings', 'gg_location', 'gg_cleanliness', 'gg_service', 'gg_value',\n",
        "                 'ex_reviews', 'ex_excellent', 'ex_very_good', 'ex_average', 'ex_poor', 'ex_terrible',\n",
        "                 'ex_ratings', 'ex_location', 'ex_cleanliness', 'ex_service', 'ex_value',\n",
        "                 'bk_reviews', 'bk_excellent', 'bk_very_good', 'bk_average', 'bk_poor', 'bk_terrible',\n",
        "                 'bk_ratings', 'bk_location', 'bk_cleanliness', 'bk_service', 'bk_value',\n",
        "                 'instagram', 'facebook', 'twitter']\n",
        "\n",
        "# Convertir a tuplas manejando pd.NA\n",
        "data = []\n",
        "for _, row in df_clean[columns_order].iterrows():\n",
        "    tuple_data = []\n",
        "    for val in row:\n",
        "        if pd.isna(val) or val is pd.NA:\n",
        "            tuple_data.append(None)\n",
        "        else:\n",
        "            tuple_data.append(val)\n",
        "    data.append(tuple(tuple_data))\n",
        "\n",
        "print(f\"Preparando inserción de {len(data)} filas...\")\n",
        "\n",
        "# Conectar a PostgreSQL\n",
        "conn = psycopg2.connect(\n",
        "    host=\"gfg-postgresql-server-2025.postgres.database.azure.com\",\n",
        "    database=\"gfg_asset_management\",\n",
        "    user=\"svc_n8n_admin\",\n",
        "    password=\"PowerBI2025!\",\n",
        "    sslmode=\"require\"\n",
        ")\n",
        "\n",
        "# SQL de inserción\n",
        "insert_sql = \"\"\"\n",
        "INSERT INTO raw_digital_presence.data_consolidado_historico\n",
        "(week, week_date, active_selection, cs, series, cs_hotels_names,\n",
        " rank_all, total, market,\n",
        " ta_reviews, ta_excellent, ta_very_good, ta_average, ta_poor, ta_terrible,\n",
        " ta_ratings, ta_location, ta_cleanliness, ta_service, ta_value,\n",
        " gg_reviews, gg_excellent, gg_very_good, gg_average, gg_poor, gg_terrible,\n",
        " gg_ratings, gg_location, gg_cleanliness, gg_service, gg_value,\n",
        " ex_reviews, ex_excellent, ex_very_good, ex_average, ex_poor, ex_terrible,\n",
        " ex_ratings, ex_location, ex_cleanliness, ex_service, ex_value,\n",
        " bk_reviews, bk_excellent, bk_very_good, bk_average, bk_poor, bk_terrible,\n",
        " bk_ratings, bk_location, bk_cleanliness, bk_service, bk_value,\n",
        " instagram, facebook, twitter)\n",
        "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,\n",
        "        %s, %s, %s)\n",
        "\"\"\"\n",
        "\n",
        "# Insertar en batch\n",
        "cursor = conn.cursor()\n",
        "print(\"Insertando datos en PostgreSQL...\")\n",
        "execute_batch(cursor, insert_sql, data, page_size=1000)\n",
        "conn.commit()\n",
        "\n",
        "print(f\"\\n✅ {len(data)} filas insertadas exitosamente en data_consolidado_historico\")\n",
        "\n",
        "# Cerrar conexión\n",
        "cursor.close()\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n🎉 Migración completada!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doUMhuXr3VUr",
        "outputId": "63339262-e2ff-4a9c-f034-c5f2825d7ce3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparando inserción de 16708 filas...\n",
            "Insertando datos en PostgreSQL...\n",
            "\n",
            "✅ 16708 filas insertadas exitosamente en data_consolidado_historico\n",
            "\n",
            "🎉 Migración completada!\n"
          ]
        }
      ]
    }
  ]
}